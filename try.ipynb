{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain_community.document_loaders import UnstructuredPowerPointLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import boto3\n",
    "import os\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from tqdm import tqdm\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from operator import itemgetter\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "import numpy as np\n",
    "from dotenv  import load_dotenv\n",
    "load_dotenv()\n",
    "import shutil\n",
    "\n",
    "def load_file(file_name):\n",
    "    loader=[]\n",
    "    print(file_name.split(\".\")[-1])\n",
    "    if file_name.split('.')[-1] == \"pptx\":\n",
    "        loader = UnstructuredPowerPointLoader(file_name).load()\n",
    "    elif file_name.split('.')[-1] == \"pdf\":\n",
    "        loader = PyPDFLoader(file_name).load()    \n",
    "    elif file_name.split('.')[-1] == \"docx\":\n",
    "        loader = Docx2txtLoader(file_name).load()\n",
    "    elif file_name.split('.')[-1] == \"html\":\n",
    "        loader = UnstructuredHTMLLoader(file_name).load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=300,\n",
    "            chunk_overlap=0,\n",
    "            is_separator_regex=False,\n",
    "        )\n",
    "    pages = text_splitter.split_documents(loader)\n",
    "    return pages\n",
    "\n",
    "\n",
    "def file_to_chunks(folder):\n",
    "    pages=[]\n",
    "    for file_name in os.listdir(f\"{folder}\"):\n",
    "        pages.extend(load_file(f\"{folder}\\\\{file_name}\"))\n",
    "    if folder != \"Local_data\":\n",
    "        shutil.rmtree(f\"{folder}\")\n",
    "    return pages\n",
    "\n",
    "def azure_data_download(AZURE_CONNECTION_STRING,CONTAINER_NAME):\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(AZURE_CONNECTION_STRING)\n",
    "    container_client = blob_service_client.get_container_client(CONTAINER_NAME)\n",
    "    if not os.path.exists(\"Azure_data\"):\n",
    "        os.mkdir(\"Azure_data\")\n",
    "    for file_name in container_client.list_blobs():\n",
    "        blob_client = container_client.get_blob_client(file_name)\n",
    "        with open(f\"Azure_data\\\\{file_name.name}\", \"wb\") as file:\n",
    "            data = blob_client.download_blob().readall()\n",
    "            file.write(data)\n",
    "\n",
    "\n",
    "def aws(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, BUCKET_NAME,object_name):\n",
    "        # Create an S3 client\n",
    "    s3 = boto3.client('s3',\n",
    "                    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "                    aws_secret_access_key=AWS_SECRET_ACCESS_KEY)\n",
    "\n",
    "    # List objects in the bucket\n",
    "    response = s3.list_objects_v2(Bucket=BUCKET_NAME)\n",
    "\n",
    "    if not os.path.exists(\"S3_data\"):\n",
    "        os.mkdir(\"S3_data\")\n",
    "\n",
    "    # Download files in the 'data' object\n",
    "    for i in response.get('Contents',[]):\n",
    "        if i['Key'].split('/')[-1] != \"\" and i['Key'].split('/')[0] == object_name:\n",
    "            # print(i['Key'])\n",
    "            file_path = os.path.join(\"S3_data\", i['Key'].split('/')[-1])\n",
    "            # print(file_path)\n",
    "            s3.download_file(BUCKET_NAME, i['Key'], file_path)\n",
    "\n",
    "\n",
    "def generate_queries(query):\n",
    "        \n",
    "    # Multi Query: Different Perspectives\n",
    "    template = \"\"\"You are an AI language model assistant. Your task is to generate Four \n",
    "    different versions of the given user question to retrieve relevant documents from a vector \n",
    "    database. By generating multiple perspectives on the user question, your goal is to help\n",
    "    the user overcome some of the limitations of the distance-based similarity search. \n",
    "    Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "    prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "    generate_querie = (\n",
    "        prompt_perspectives \n",
    "        | ChatOpenAI(temperature=0) \n",
    "        | StrOutputParser() \n",
    "        | (lambda x: x.split(\"\\n\"))\n",
    "        | (lambda x: [query] + x)\n",
    "    )\n",
    "    return generate_querie \n",
    "\n",
    "\n",
    "def _get_docs(m):\n",
    "    docs=[]\n",
    "    docs.extend(m['original'])\n",
    "    for i in m['new']:\n",
    "        docs.extend(i)\n",
    "    return docs\n",
    "\n",
    "def keyword_extractor():\n",
    "    prompt=\"\"\"\n",
    "    You are an AI language model assistant. Your task is to help the user identify key terms in their query.\n",
    "\n",
    "    Please list the main keywords you want to extract from your query.\n",
    "\n",
    "    query: {query}\n",
    "    \"\"\"\n",
    "    prompt_perspectives=ChatPromptTemplate.from_template(prompt)\n",
    "    generate_querie = (\n",
    "        prompt_perspectives \n",
    "        | ChatOpenAI(temperature=0) \n",
    "        | StrOutputParser() )\n",
    "    return generate_querie\n",
    "\n",
    "def _get(a):\n",
    "    dd=[]\n",
    "    for s in a:\n",
    "        dd.extend(s)\n",
    "    return dd\n",
    "\n",
    "def get_unique_documents(doc_list):\n",
    "    seen_content = set()\n",
    "    unique_documents = []\n",
    "    \n",
    "    for doc in doc_list:\n",
    "        content = doc.page_content\n",
    "        if content not in seen_content:\n",
    "            seen_content.add(content)\n",
    "            unique_documents.append(doc)\n",
    "    \n",
    "    del seen_content\n",
    "    \n",
    "    return unique_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pptx\n",
      "pdf\n",
      "pdf\n",
      "docx\n",
      "docx\n",
      "html\n",
      "docx\n"
     ]
    }
   ],
   "source": [
    "pages = file_to_chunks(\"Local_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "db = Chroma.from_documents(pages, OpenAIEmbeddings(), persist_directory=\"Local_Chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Query=\"What is mlflow?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "faiss_retriever=db.as_retriever(search_kwargs={'k': 10})\n",
    "\n",
    "Bm25_retriever = BM25Retriever.from_documents(pages)\n",
    "Bm25_retriever.k = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_chain=generate_queries | faiss_retriever.map() | _get | get_unique_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_chain=keyword_extractor() | Bm25_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_retriever = EnsembleRetriever(\n",
    "retrievers=[map_chain, key_chain], weights=[0.25, 0.75]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shyams\\Downloads\\RAG with different source\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\shyams\\Downloads\\RAG with different source\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\shyams\\Downloads\\RAG with different source\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = HuggingFaceCrossEncoder(model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "compressor = CrossEncoderReranker(model=model, top_n=4)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=ensemble_retriever\n",
    ")\n",
    "\n",
    "final_prompt=\"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "    Question: {question} \n",
    "    Context: {context} \n",
    "    Answer:\"\"\"\n",
    "\n",
    "final_prompt_perspectives=ChatPromptTemplate.from_template(final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm_chain2=({\"context\": itemgetter(\"query\") | compression_retriever,\n",
    "        \"question\":itemgetter(\"query\")}\n",
    "        | final_prompt_perspectives\n",
    "        | ChatOpenAI(temperature=0) | StrOutputParser() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "        \n",
    "llm_chain3= ({\"context\": itemgetter(\"query\") | compression_retriever,\n",
    "            \"question\":itemgetter(\"query\")}\n",
    "            | \n",
    "            RunnableParallel({\n",
    "                \"response\":  final_prompt_perspectives | ChatOpenAI(temperature=0) | StrOutputParser() ,\n",
    "                \"context\": itemgetter(\"context\")\n",
    "            })\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  context: RunnableLambda(itemgetter('query'))\n",
       "           | ContextualCompressionRetriever(base_compressor=CrossEncoderReranker(model=HuggingFaceCrossEncoder(client=<sentence_transformers.cross_encoder.CrossEncoder.CrossEncoder object at 0x00000290D21CAD80>, model_name='cross-encoder/ms-marco-MiniLM-L-6-v2', model_kwargs={}), top_n=4), base_retriever=EnsembleRetriever(retrievers=[RunnableLambda(generate_queries)\n",
       "             | RunnableEach(bound=VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x00000290D50738F0>, search_kwargs={'k': 10}))\n",
       "             | RunnableLambda(_get)\n",
       "             | RunnableLambda(get_unique_documents), ChatPromptTemplate(input_variables=['query'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], template='\\n    You are an AI language model assistant. Your task is to help the user identify key terms in their query.\\n\\n    Please list the main keywords you want to extract from your query.\\n\\n    query: {query}\\n    '))])\n",
       "             | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x00000290D22248F0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x00000290D2A55A30>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')\n",
       "             | StrOutputParser()\n",
       "             | BM25Retriever(vectorizer=<rank_bm25.BM25Okapi object at 0x00000290D64B10D0>, k=10)], weights=[0.25, 0.75])),\n",
       "  question: RunnableLambda(itemgetter('query'))\n",
       "}\n",
       "| {\n",
       "    response: ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\n    Question: {question} \\n    Context: {context} \\n    Answer:\"))])\n",
       "              | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x00000290F0AD0650>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x00000290F0C2B9E0>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')\n",
       "              | StrOutputParser(),\n",
       "    context: RunnableLambda(itemgetter('context'))\n",
       "  }"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=llm_chain3.invoke({\"query\":Query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta(s):\n",
    "    f=[]\n",
    "    for i in s:\n",
    "        f.append(f\"Page : {i.metadata['page']} , Source : {i.metadata['source']}\")\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MLflow is an open-source platform for the machine learning life cycle, focusing on reproducibility, training, and deployment. It provides facilities for experiment tracking, model management, registry, and deployment interface. MLflow allows practitioners to manage the ML life cycle from model development to deployment in a reliable and scalable environment.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(meta(s[\"context\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[\"context\"][0].metadata[\"page\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Page : 134 , Source : Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf',\n",
       " 'Page : 19 , Source : Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf',\n",
       " 'Page : 64 , Source : Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf',\n",
       " 'Page : 20 , Source : Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta(s[\"context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metadata(s):\n",
    "    for i in s:\n",
    "        print(i.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page': 134, 'source': 'Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf'}\n",
      "{'page': 19, 'source': 'Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf'}\n",
      "{'page': 64, 'source': 'Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf'}\n",
      "{'page': 20, 'source': 'Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf'}\n"
     ]
    }
   ],
   "source": [
    "metadata(s[\"context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MLflow is an open-source platform for the machine learning life cycle, focusing on reproducibility, training, and deployment. It provides facilities for experiment tracking, model management, registry, and deployment interface. MLflow allows practitioners to manage the ML life cycle from model development to deployment in a reliable and scalable environment.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain2.invoke({\"query\":Query})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
