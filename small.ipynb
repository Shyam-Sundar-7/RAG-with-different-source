{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain_community.document_loaders import UnstructuredPowerPointLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import boto3\n",
    "import os\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from tqdm import tqdm\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from operator import itemgetter\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "import numpy as np\n",
    "from dotenv  import load_dotenv\n",
    "load_dotenv()\n",
    "import shutil\n",
    "\n",
    "def load_file(file_name):\n",
    "    loader=[]\n",
    "    # print(file_name.split(\".\")[-1])\n",
    "    if file_name.split('.')[-1] == \"pptx\":\n",
    "        loader = UnstructuredPowerPointLoader(file_name).load()\n",
    "    elif file_name.split('.')[-1] == \"pdf\":\n",
    "        loader = PyPDFLoader(file_name).load()    \n",
    "    elif file_name.split('.')[-1] == \"docx\":\n",
    "        loader = Docx2txtLoader(file_name).load()\n",
    "    elif file_name.split('.')[-1] == \"html\":\n",
    "        loader = UnstructuredHTMLLoader(file_name).load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=300)\n",
    "    pages = text_splitter.split_documents(loader)\n",
    "    return pages\n",
    "\n",
    "\n",
    "def file_to_chunks(folder):\n",
    "    pages=[]\n",
    "    for file_name in os.listdir(f\"{folder}\"):\n",
    "        pages.extend(load_file(f\"{folder}\\\\{file_name}\"))\n",
    "    if folder != \"Local_data\":\n",
    "        shutil.rmtree(f\"{folder}\")\n",
    "    return pages\n",
    "\n",
    "def azure_data_download(AZURE_CONNECTION_STRING,CONTAINER_NAME):\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(AZURE_CONNECTION_STRING)\n",
    "    container_client = blob_service_client.get_container_client(CONTAINER_NAME)\n",
    "    if not os.path.exists(\"Azure_data\"):\n",
    "        os.mkdir(\"Azure_data\")\n",
    "    for file_name in container_client.list_blobs():\n",
    "        blob_client = container_client.get_blob_client(file_name)\n",
    "        with open(f\"Azure_data\\\\{file_name.name}\", \"wb\") as file:\n",
    "            data = blob_client.download_blob().readall()\n",
    "            file.write(data)\n",
    "\n",
    "\n",
    "def aws(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, BUCKET_NAME,object_name):\n",
    "        # Create an S3 client\n",
    "    s3 = boto3.client('s3',\n",
    "                    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "                    aws_secret_access_key=AWS_SECRET_ACCESS_KEY)\n",
    "\n",
    "    # List objects in the bucket\n",
    "    response = s3.list_objects_v2(Bucket=BUCKET_NAME)\n",
    "\n",
    "    if not os.path.exists(\"S3_data\"):\n",
    "        os.mkdir(\"S3_data\")\n",
    "\n",
    "    # Download files in the 'data' object\n",
    "    for i in response.get('Contents',[]):\n",
    "        if i['Key'].split('/')[-1] != \"\" and i['Key'].split('/')[0] == object_name:\n",
    "            # print(i['Key'])\n",
    "            file_path = os.path.join(\"S3_data\", i['Key'].split('/')[-1])\n",
    "            # print(file_path)\n",
    "            s3.download_file(BUCKET_NAME, i['Key'], file_path)\n",
    "\n",
    "def generate_queries_with_history():\n",
    "    import json\n",
    "    with open(\"history.json\", \"rb\") as f:\n",
    "        h_1 = json.load(f)\n",
    "    h_1=formating_history(h_1)\n",
    "\n",
    "    prompt = \"\"\"You are an AI language model assistant. Your task is to generate 5 different versions of the given user question to retrieve relevant documents from a vector database. By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search.\n",
    "        Additionally, look for the relevant information from the history to provide more contextually accurate variations. Provide these alternative questions separated by newlines.\n",
    "        \n",
    "        history : \"\"\"+ h_1\n",
    "    template=prompt+\"\"\"\n",
    "        Original question: {query}\n",
    "        \"\"\"\n",
    "    # print(template)\n",
    "    # print(h_1)\n",
    "    prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "    print(prompt_perspectives)\n",
    "    generate_querie = (\n",
    "        prompt_perspectives\n",
    "        | ChatOpenAI(temperature=0) \n",
    "        | StrOutputParser() \n",
    "        | (lambda x: x.split(\"\\n\"))\n",
    "        # | (lambda x: [query] + x)\n",
    "    )\n",
    "    return generate_querie \n",
    "\n",
    "def _get(a):\n",
    "    dd=[]\n",
    "    for s in a:\n",
    "        dd.extend(s)\n",
    "    return dd\n",
    "\n",
    "def get_unique_documents(doc_list):\n",
    "    seen_content = set()\n",
    "    unique_documents = []\n",
    "    \n",
    "    for doc in doc_list:\n",
    "        content = doc.page_content\n",
    "        if content not in seen_content:\n",
    "            seen_content.add(content)\n",
    "            unique_documents.append(doc)\n",
    "    \n",
    "    del seen_content\n",
    "    \n",
    "    return unique_documents\n",
    "\n",
    "\n",
    "def keyword_extractor_with_history():\n",
    "    import json\n",
    "    with open(\"history.json\", \"rb\") as f:\n",
    "        h_1 = json.load(f)\n",
    "    h_1=formating_history(h_1)\n",
    "    prompt=\"\"\"\n",
    "    You are an AI language model assistant. Your task is to help the user identify key terms in their query.\n",
    "    Please list the main keywords you want to extract from your query.\n",
    "\n",
    "    Additionally, look for the relevant information from the history to provide more contextually accurate variations.\n",
    "    history : \"\"\" + h_1\n",
    "\n",
    "    tem=prompt +\"\"\"\n",
    "    query: {query}\n",
    "    \"\"\"\n",
    "    prompt_perspectives=ChatPromptTemplate.from_template(tem)\n",
    "    print(prompt_perspectives)\n",
    "    generate_querie = (\n",
    "        prompt_perspectives \n",
    "        | ChatOpenAI(temperature=0) \n",
    "        | StrOutputParser() )\n",
    "    return generate_querie\n",
    "\n",
    "def main(Query,chunks,db):\n",
    "    \n",
    "    faiss_retriever=db.as_retriever(search_kwargs={'k': 10})\n",
    "\n",
    "    Bm25_retriever = BM25Retriever.from_documents(chunks)\n",
    "    Bm25_retriever.k = 10\n",
    "\n",
    "    map_chain=generate_queries_with_history | faiss_retriever.map() | _get | get_unique_documents\n",
    "    key_chain=keyword_extractor_with_history() | Bm25_retriever | get_unique_documents\n",
    "\n",
    "    ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[map_chain, key_chain], weights=[0.5, 0.5]\n",
    "    )\n",
    "\n",
    "    model = HuggingFaceCrossEncoder(model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "    compressor = CrossEncoderReranker(model=model, top_n=4)\n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor=compressor, base_retriever=ensemble_retriever\n",
    "    )\n",
    "\n",
    "    final_prompt=\"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "        Question: {question} \n",
    "        Context: {context} \n",
    "        Answer:\"\"\"\n",
    "\n",
    "    final_prompt_perspectives=ChatPromptTemplate.from_template(final_prompt)\n",
    "\n",
    "    llm_chain= ({\"context\": itemgetter(\"query\") | compression_retriever,\n",
    "            \"question\":itemgetter(\"query\")}\n",
    "            | \n",
    "            RunnableParallel({\n",
    "                \"response\":  final_prompt_perspectives | ChatOpenAI(temperature=0) | StrOutputParser() ,\n",
    "                \"context\": itemgetter(\"context\")\n",
    "            })\n",
    "            )\n",
    "    \n",
    "    return llm_chain.invoke({\"query\":Query})\n",
    "\n",
    "def formating_history(h):\n",
    "    d=\"\"\n",
    "    for i in h:\n",
    "        for _,j in i.items():\n",
    "            d=d+f\"{j} \\n\"\n",
    "    if d == \"\":\n",
    "        d = \"No history found\"\n",
    "    return d\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"history.json\", \"rb\") as f:\n",
    "    h_1 = json.load(f)\n",
    "h_1=formating_history([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['query'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], template='\\n    You are an AI language model assistant. Your task is to help the user identify key terms in their query.\\n    Please list the main keywords you want to extract from your query.\\n\\n    Additionally, look for the relevant information from the history to provide more contextually accurate variations.\\n    history : No history found\\n    query: {query}\\n    '))]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Keywords:\\n1. Tell\\n2. About\\n3. Yourself'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_extractor_with_history().invoke({\"query\":\"Tell me about yourself\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['query'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], template='You are an AI language model assistant. Your task is to generate 5 different versions of the given user question to retrieve relevant documents from a vector database. By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search.\\n        Additionally, look for the relevant information from the history to provide more contextually accurate variations. Provide these alternative questions separated by newlines.\\n        \\n        history : No history found\\n        Original question: {query}\\n        '))]\n"
     ]
    }
   ],
   "source": [
    "s=generate_queries_with_history().invoke({\"query\":\"What is the capital of India?\"})\n",
    "# .invoke({\"query\":\"What is the capital of India?\",\"history\":h_1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. Can you tell me the capital city of India?',\n",
       " '2. Which city serves as the capital of India?',\n",
       " '3. What is the official capital of India?',\n",
       " '4. Do you know the name of the capital city in India?',\n",
       " '5. Could you provide information on the capital of India?']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=keyword_extractor_with_history().invoke({\"query\":\"What is the capital of India?\",\"history\":h_1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Keywords:\\n1. Capital\\n2. India'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from langchain_community.vectorstores import FAISS\n",
    "with open(\"pages.pkl\", \"rb\") as f:\n",
    "    chunks = pickle.load(f)\n",
    "\n",
    "\n",
    "db=FAISS.load_local(\"Local_vectorstore\",OpenAIEmbeddings(),allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['history', 'query'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['history', 'query'], template='You are an AI language model assistant. Your task is to generate 5 different versions of the given user question to retrieve relevant documents from a vector database. By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search.\\n        Additionally, look for the relevant information from the history to provide more contextually accurate variations. Provide these alternative questions separated by newlines.\\n        \\n        history : {history}\\n        Original question: {query}\\n        '))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shyams\\Downloads\\RAG with different source\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "faiss_retriever=db.as_retriever(search_kwargs={'k': 10})\n",
    "\n",
    "Bm25_retriever = BM25Retriever.from_documents(chunks)\n",
    "Bm25_retriever.k = 10\n",
    "\n",
    "map_chain=generate_queries_with_history() | faiss_retriever.map() | _get | get_unique_documents\n",
    "key_chain=keyword_extractor_with_history() | Bm25_retriever | get_unique_documents\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "retrievers=[map_chain, key_chain], weights=[0.5, 0.5]\n",
    ")\n",
    "\n",
    "model = HuggingFaceCrossEncoder(model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "compressor = CrossEncoderReranker(model=model, top_n=4)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=ensemble_retriever\n",
    ")\n",
    "\n",
    "final_prompt=\"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "    Question: {question} \n",
    "    Context: {context} \n",
    "    Answer:\"\"\"\n",
    "\n",
    "final_prompt_perspectives=ChatPromptTemplate.from_template(final_prompt)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=ensemble_retriever.invoke({\"query\":\"What is the capital of India?\",\"history\":h_1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shyams\\Downloads\\RAG with different source\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ContextualCompressionRetriever\nbase_retriever\n  Can't instantiate abstract class BaseRetriever without an implementation for abstract method '_get_relevant_documents' (type=type_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[93], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m HuggingFaceCrossEncoder(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcross-encoder/ms-marco-MiniLM-L-6-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m compressor \u001b[38;5;241m=\u001b[39m CrossEncoderReranker(model\u001b[38;5;241m=\u001b[39mmodel, top_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m compression_retriever \u001b[38;5;241m=\u001b[39m \u001b[43mContextualCompressionRetriever\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_compressor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompressor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_retriever\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mm\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shyams\\Downloads\\RAG with different source\\.venv\\Lib\\site-packages\\pydantic\\v1\\main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[1;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for ContextualCompressionRetriever\nbase_retriever\n  Can't instantiate abstract class BaseRetriever without an implementation for abstract method '_get_relevant_documents' (type=type_error)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ContextualCompressionRetriever\nbase_retriever\n  Can't instantiate abstract class BaseRetriever without an implementation for abstract method '_get_relevant_documents' (type=type_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[94], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m HuggingFaceCrossEncoder(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcross-encoder/ms-marco-MiniLM-L-6-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m compressor \u001b[38;5;241m=\u001b[39m CrossEncoderReranker(model\u001b[38;5;241m=\u001b[39mmodel, top_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m compression_retriever \u001b[38;5;241m=\u001b[39m \u001b[43mContextualCompressionRetriever\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_compressor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompressor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_retriever\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mm\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shyams\\Downloads\\RAG with different source\\.venv\\Lib\\site-packages\\pydantic\\v1\\main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[1;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for ContextualCompressionRetriever\nbase_retriever\n  Can't instantiate abstract class BaseRetriever without an implementation for abstract method '_get_relevant_documents' (type=type_error)"
     ]
    }
   ],
   "source": [
    "m={\"query\":itemgetter(\"query\") , \"history\":itemgetter(\"history\")} | ensemble_retriever\n",
    "\n",
    "model = HuggingFaceCrossEncoder(model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "compressor = CrossEncoderReranker(model=model, top_n=4)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=m\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"About the reviewer\\nHitesh Hinduja is an ardent AI enthusiast working as a Senior Manager in AI at Ola \\nElectric, where he leads a team of 20+ people in the areas of machine learning, deep \\nlearning, statistics, computer vision, natural language processing, and reinforcement \\nlearning. He has filed 14+ patents in India and the US and has numerous research \\npublications under his name. Hitesh has been associated in research roles at India's \\ntop B-schools: Indian School of Business, Hyderabad, and the Indian Institute of \\nManagement, Ahmedabad. He is also actively involved in training and mentoring and has \\nbeen invited as a guest speaker by various corporates and associations across the globe.\", metadata={'source': 'Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf', 'page': 4}),\n",
       " Document(page_content='1. Install evidently :\\npip install evidently==0.1.17.dev0\\n2. Import the relevant libraries:\\nimport pandas as pd\\nimport numpy as np\\nfrom sklearn import datasets\\nfrom sklearn.model_selection import train_test_split\\nfrom evidently.dashboard import Dashboard\\nfrom evidently.tabs import DataDriftTab, \\nNumTargetDriftTab,CatTargetDriftTab', metadata={'source': 'Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf', 'page': 204}),\n",
       " Document(page_content='Machine Learning \\nEngineering  \\nwith MLflow\\nManage the end-to-end machine learning life cycle \\nwith MLflow\\nNatu Lauchande\\nBIRMINGHAM—MUMBAI', metadata={'source': 'Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf', 'page': 1}),\n",
       " Document(page_content='1. We will start by importing the relevant packages:\\nimport mlflow\\nfrom datetime import date\\nfrom dateutil.relativedelta import relativedelta\\nimport pprint\\nimport pandas\\nimport pandas_datareader.data as web\\n2. Next, you should add a function to retrieve the data: \\nif __name__ == \"__main__\":\\n    \\n    with mlflow.start_run(run_name=\"load_raw_data\") as', metadata={'source': 'Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf', 'page': 143}),\n",
       " Document(page_content='Shyam Sundar\\n5,kavimani steet, Pankajam colony, 3rd cross street, Madurai, India - 625009\\n♂phone+91-9080765574 /envel⌢pemailshyamsundar.2022@gmail.com /linkedinshyamsundar007 /githubShyam-Sundar-7♂laptopPortfolio\\nProjects\\nLLM-Powered Coupon Recommender /github/video |Python, Streamlit, Langchain, OpenAI November 2023\\n•Developed a QA system for e-commerce with personalized coupon recommendations using OpenAI’s LLMs.\\n•Streamlined user interactions through a Streamlit interface and Langchain for real-world scenario simulations.\\n•Incorporated FAISS for refined recommendation processes.\\nPeopleCare Insurance Prediction /github |Python, Jupyter, Azure Cloud, Flask, Docker October 2023\\n•Expanded PeopleCare into vehicle insurance with a predictive model for effective customer targeting.\\n•Thorough analysis of customer behavior and data cleaning for accurate predictive modeling.\\n•Achieved 80% prediction accuracy using LightGBM.', metadata={'source': 'Local_data\\\\resume.pdf', 'page': 0}),\n",
       " Document(page_content='project dataset. We will execute the following steps in order to implement AutoML for our \\nuse case:\\n1. Let\\'s start by installing the full version of PyCaret, as follows:\\npip install pycaret==2.3.1\\n2. First, we should import the necessary libraries, like so:\\nimport pandas\\nimport pycaret\\n3. Then, we read all the training data, like this:\\ndata=pandas.read_csv(\"training_data.csv\",header=\\'infer\\')', metadata={'source': 'Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf', 'page': 219}),\n",
       " Document(page_content='this book.\\nPackt Publishing has endeavored to provide trademark information about all of the companies \\nand products mentioned in this book by the appropriate use of capitals. However, Packt Publishing \\ncannot guarantee the accuracy of this information.\\nPublishing Product Manager: Reshma Raman\\nSenior Editor: David Sugarman\\nContent Development Editor : Sean Lobo\\nTechnical Editor: Manikandan Kurup\\nCopy Editor: Safis Editing\\nProject Coordinator: Aparna Ravikumar Nair\\nProofreader : Safis Editing\\nIndexer : Pratik Shirodkar\\nProduction Designer: Sinhayna Bais\\nFirst published: August 2021\\nProduction reference: 1220721\\nPublished by Packt Publishing Ltd.\\nLivery Place\\n35 Livery Street\\nBirmingham\\nB3 2PB, UK.\\nISBN 978-1-80056-079-6\\nwww.packt.com', metadata={'source': 'Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf', 'page': 2}),\n",
       " Document(page_content='1. Y ou need to set up a Docker file in the root folder of the project, as shown in the \\nfollowing code snippet:\\nFROM continuumio/miniconda3:4.9.2\\nRUN apt-get update && apt-get install build-essential -y\\nRUN pip install \\\\\\n    mlflow==1.18.0 \\\\\\n    pymysql==1.0.2 \\\\\\n    boto3\\nCOPY ./training_project /src\\nWORKDIR /src\\n2. We will start by building and training the image by running the following command:\\ndocker build -t psystock_docker_training_image .', metadata={'source': 'Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf', 'page': 169}),\n",
       " Document(page_content='•Advanced Numerical Techniques•Machine Learning\\n•Data Science•Deep Learning\\n•Computer Graphics\\nTechnical Skills\\nLanguages : Python, C/C++, SQL (Postgres), Matlab, Latex\\nFrameworks : Pytorch, Tensorflow, Flask, Pytorch Lightning\\nTools/Platform : Tableau, Power Bi, Azure, Git, Jupyter, Docker\\nLibraries : Scikit-Learn, Pandas, Numpy, Matplotlib, Seaborn\\nCertifications\\n•Certified Associate Data Analyst\\n•SQL [Advanced] - Hackerank\\n•Management Consulting Mentorship\\n•Generative AI at SAP•ML for Business professionals using No-Code AI tools\\n•Python [Basic] - Hackerank\\n•Software Engineer Intern -Hackerank\\nPublications\\nSuspicious Event Detection of Cargo Vessels Based on AIS Data at ICDMAI,2023', metadata={'source': 'Local_data\\\\resume.pdf', 'page': 0}),\n",
       " Document(page_content='1. Copy the contents of the project available in https://github.com/\\nPacktPublishing/Machine-Learning-Engineering-with-MLflow/\\ntree/master/Chapter03/gradflow .\\n2. Start your local environment by running the following command:\\nmake\\n3. Inspect the created environments, like this: \\n$ docker ps \\nThe following screenshot presents three Docker images: the first for Jupyter, the \\nsecond for MLflow, and the third for the PostgreSQL database. The status should \\nshow Up x minutes :\\nFigure 3.3 – Running Docker images', metadata={'source': 'Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf', 'page': 67}),\n",
       " Document(page_content='Enter the main heading, usually the same as the title.\\n\\nBe bold in stating your key points. Put them in a list:\\n\\nThe first item in your list\\n\\nThe second item; italicize key words\\n\\nImprove your image by including an image.\\n\\nAdd a link to your favorite Web site.\\nBreak up your page with a horizontal rule or two.\\n\\nFinally, link to another page in your own Web site.\\n\\n© Wiley Publishing, 2011', metadata={'source': 'Local_data\\\\sample2.html'}),\n",
       " Document(page_content='A very popular managed ML and data platform is the Databricks platform, developed \\nby the same company that developed MLflow. We will use in this section the Databricks \\nCommunity Edition version and license targeted for students and personal use. \\nIn order to explore the Databricks platform to develop and share models, you need to \\nexecute the following steps:\\n1. Sign up to Databricks Community Edition at https://community.cloud.\\ndatabricks.com/  and create an account.\\n2. Log in to your account with your just-created credentials.', metadata={'source': 'Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf', 'page': 188}),\n",
       " Document(page_content='•Central global model collected weight updates from six randomly selected client models, averaging the contributions, and\\ndisseminated the updated global model to all participating clients.\\n•Effected a commendable accuracy rate of 78% upon successful completion of the training process, demonstrating the effectiveness\\nof the federated learning approach in preserving data security and privacy while maintaining model performance.\\nEducation\\nDefence Institute of Advanced Technology Pune, IN\\nM.Tech in Modelling and Simulation, GPA: 7.95 May 2023\\nNational Institute of Technology Tiruchirappalli, IN\\nB.Tech in Chemical Engineering, GPA: 7.65 May 2021\\nRelevant Coursework\\n•Data Structures\\n•Advanced Numerical Techniques•Machine Learning\\n•Data Science•Deep Learning\\n•Computer Graphics\\nTechnical Skills\\nLanguages : Python, C/C++, SQL (Postgres), Matlab, Latex\\nFrameworks : Pytorch, Tensorflow, Flask, Pytorch Lightning\\nTools/Platform : Tableau, Power Bi, Azure, Git, Jupyter, Docker', metadata={'source': 'Local_data\\\\resume.pdf', 'page': 0}),\n",
       " Document(page_content='Monitoring data drift and model performance      191\\nMonitoring target drift\\nWe will now compare the scored output with the reference training output to look for \\npossible target drift:\\n1. Get the recently scored dataset:\\nproduction_scored_data = \\\\\\npd.read_csv(\"scored_data.csv\", header=None,\\n            names=[ \"day{}\".format(i) for i in \\\\\\n                    range(0,14) ]+[\"target\"] )\\nbcancer_data_and_target_drift = \\\\\\nDashboard(reference_data, production_scored_data,\\n          tabs=[ CatTargetDriftTab])\\nbcancer_data_and_target_drift.save(\\'reports/target_drift.\\nhtml\\')\\n2. Execute the data drift report generation and log the results in MLflow:\\nEXPERIMENT_NAME=\"./reports_target_drift\"\\nmlflow.set_experiment(EXPERIMENT_NAME)\\nwith mlflow.start_run():\\n    model_target_drift = \\\\\\n    Dashboard(reference_data, production_scored_data,\\n              tabs=[CatTargetDriftTab])\\n    model_target_drift.save(EXPERIMENT_NAME+\"/target_\\ndrift.html\")\\n    drift_dashboard._save_to_json(EXPERIMENT_NAME+\"/', metadata={'source': 'Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf', 'page': 207}),\n",
       " Document(page_content='Quantum Aristoxeni ingenium consumptum videmus in musicis?\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Quid nunc honeste dicit? Tum Torquatus: Prorsus, inquit, assentior; Duo Reges: constructio interrete. Iam in altera philosophiae parte. Sed haec omittamus; Haec para/doca illi, nos admirabilia dicamus. Nihil sane.\\n\\nExpressa vero in iis aetatibus, quae iam confirmatae sunt.\\n\\nSit sane ista voluptas. Non quam nostram quidem, inquit Pomponius iocans; An tu me de L. Sed haec omittamus; Cave putes quicquam esse verius.', metadata={'source': 'Local_data\\\\sample2.docx'}),\n",
       " Document(page_content='modifications to make it conform to the Docker format acceptable to the MLflow \\ndeployment in production through Kubernetes. The prerequisite of this section is that you \\nhave access to a Kubernetes cluster or can set up a local one. Guides for this can be found \\nat https://kind.sigs.k8s.io/docs/user/quick-start/  or https://\\nminikube.sigs.k8s.io/docs/start/ . \\nY ou will now execute the following steps to deploy your model from the registry  \\nin Kubernetes:\\n1. Prerequisite: Deploy and configure kubectl  (https://kubernetes.io/\\ndocs/reference/kubectl/overview/ ) and link it to your Kubernetes \\ncluster.\\n2. Create a Kubernetes backend configuration file:\\n{\\n  \"kube-context\": \"docker-for-desktop\",\\n  \"repository-uri\": \"username/mlflow-kubernetes-example\",\\n  \"kube-job-template-path\": \"/Users/username/path/to/\\nkubernetes_job_template.yaml\"\\n}', metadata={'source': 'Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf', 'page': 177}),\n",
       " Document(page_content='Preface     xv\\nGet in touch\\nFeedback from our readers is always welcome.\\nGeneral feedback: If you have questions about any aspect of this book, email us at \\ncustomercare@packtpub.com and mention the book title in the subject of your message.\\nErrata: Although we have taken every care to ensure the accuracy of our content, mistakes \\ndo happen. If you have found a mistake in this book, we would be grateful if you would \\nreport this to us. Please visit www.packtpub.com/support/errata  and fill in  \\nthe form.\\nPiracy: If you come across any illegal copies of our works in any form on the internet, \\nwe would be grateful if you would provide us with the location address or website name. \\nPlease contact us at copyright@packt.com with a link to the material.\\nIf you are interested in becoming an author: If there is a topic that you have expertise  \\nin and you are interested in either writing or contributing to a book, please visit \\nauthors.packtpub.com .\\nShare Your Thoughts', metadata={'source': 'Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf', 'page': 15}),\n",
       " Document(page_content='def __init__(self):\\n    pass\\n  def predict(self, context, model_input):\\n    return model_input.apply(lambda column: random.\\nrandint(0,1))\\nUnder the notebook  folder in the notebooks/stockpred_randomizer.ipynb  \\nfile, you can follow along with the integration of the preceding code excerpt in our \\nrecently created data science workbench. We will proceed as follows:\\n1. We will first import all the dependencies needed and run the first cell of the \\nnotebook, as follows:\\nFigure 3.9 – MLflow experiment details\\n2. Let’s declare and execute the class outlined in Figure 3.9, represented in the second \\ncell of the notebook, as follows: \\nFigure 3.10 – Notebook cell with the RandomPredictor class declaration', metadata={'source': 'Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf', 'page': 71}),\n",
       " Document(page_content='City or Town\\n\\nPoint A\\n\\nPoint B\\n\\nPoint C\\n\\nPoint D\\n\\nPoint E\\n\\nPoint A\\n\\n—\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPoint B\\n\\n87\\n\\n—\\n\\n\\n\\n\\n\\n\\n\\nPoint C\\n\\n64\\n\\n56\\n\\n—\\n\\n\\n\\n\\n\\nPoint D\\n\\n37\\n\\n32\\n\\n91\\n\\n—\\n\\n\\n\\nPoint E\\n\\n93\\n\\n35\\n\\n54\\n\\n43\\n\\n—\\n\\n\\n\\nNext, we see a table with special formatting in various locations. Notice how the formatting for the header row and sub header rows is preserved.\\n\\nCollege\\n\\nNew students\\n\\nGraduating students\\n\\nChange\\n\\n\\n\\nUndergraduate\\n\\n\\n\\n\\n\\nCedar University\\n\\n110\\n\\n103\\n\\n+7\\n\\nOak Institute\\n\\n202\\n\\n210\\n\\n-8\\n\\n\\n\\nGraduate\\n\\n\\n\\n\\n\\nCedar University\\n\\n24\\n\\n20\\n\\n+4\\n\\nElm College\\n\\n43\\n\\n53\\n\\n-10\\n\\nTotal\\n\\n998\\n\\n908\\n\\n90\\n\\nSource: Fictitious data, for illustration purposes only\\n\\nNext, we have something a little more complex, a nested table, i.e. a table inside another table. Additionally, the inner table has some of its cells merged. The table is displayed horizontally centered.\\n\\nOne\\n\\nThree\\n\\nTwo\\n\\n\\n\\nFour\\n\\n\\n\\nTo the left is a table inside a table, with some cells merged.', metadata={'source': 'Local_data\\\\sample1.docx'}),\n",
       " Document(page_content='136     Data and Feature Management\\nIn order to understand how Feast works and how it can fit into your data layer component \\n(code available at https://github.com/PacktPublishing/Machine-\\nLearning-Engineering-with-MLflow/tree/master/Chapter07/\\npsystock_feature_store , execute the following steps:\\n1. Install feast :\\npip install feast==0.10\\n2. Initialize a feature repository:\\nfeast init\\n3. Create your feature definitions by replacing the yaml  file generated automatically:\\nproject: psystock_feature_store\\nregistry: data/registry.db\\nprovider: local\\nonline_store:\\n    path: data/online_store.db\\n4. We will now proceed to import dependencies of the feature definition:\\nfrom google.protobuf.duration_pb2 import Duration\\nfrom feast import Entity, Feature, FeatureView, ValueType\\nfrom feast.data_source import FileSource\\n5. We can now load the feature files:\\ntoken_features = FileSource(\\n    path=\"/data/features.csv\",\\n    event_timestamp_column=\"create_date\",', metadata={'source': 'Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf', 'page': 152}),\n",
       " Document(page_content='and real-time world and the existence of generic prediction services for API and  \\nbatch systems:\\nFigure.6.3 – Michelangelo architecture \\nThe components that we choose to use as a reference for architecture comparison are  \\nthe following:\\n• Data and feature management: It consists of a centralized data store with all the \\nfeatures that are needed to serve models and train models. The feature data store \\ncan be accessed in real time and in batch. For the batch scenario, they use a database \\nsystem called Hive and for real time, they use Cassandra.', metadata={'source': 'Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf', 'page': 128}),\n",
       " Document(page_content='on-premises, in the cloud, or local.\\nWe have previously defined the business requirements of the prediction use cases, namely \\ndetection of the movement of cryptocurrency and value prediction. To leverage this and \\nother use cases, the creation of an ML platform is critical to the company.', metadata={'source': 'Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf', 'page': 131}),\n",
       " Document(page_content='3\\nYour Data Science \\nWorkbench\\nIn this chapter, you will learn about MLflow in the context of creating a local environment \\nso that you can develop your machine learning project locally with the different features \\nprovided by MLflow. This chapter is focused on machine learning engineering, and one \\nof the most important roles of a machine learning engineer is to build up an environment \\nwhere model developers and practitioners can be efficient. We will also demonstrate a \\nhands-on example of how we can use workbenches to accomplish specific tasks.\\nSpecifically,\\xa0we will look at the following\\xa0topics in this chapter:\\xa0\\n• Understanding the value of a data science workbench\\n• Creating your own data science workbench\\n• Using the workbench for stock prediction', metadata={'source': 'Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf', 'page': 59}),\n",
       " Document(page_content=\"develop our ML platform in the PsyStock company.\\nBased on the work done so far in prototyping models to predict the price of Bitcoin, the \\nbusiness development department of the company decided to start its first product as a \\nPrediction API for cryptocurrencies as they are becoming a popular technology in the \\ncorporate world. A team was assembled that decided to investigate challenges and state-\\nof-the-art platforms, and then architect the company's own platform.\", metadata={'source': 'Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf', 'page': 123}),\n",
       " Document(page_content='Index   227\\nH\\nHadoop Distributed File \\nSystem (HDFS)  216\\nhyperparameter optimization\\nused, for tuning logistic regression-\\nbased model  79-83\\nHyperText Transfer Protocol (HTTP)  216\\nI\\ninference\\nused, for creating API process  160, 161\\nInformation Technology (IT)  209\\ninfrastructure monitoring and \\nalerting components\\nalerting  198\\nresource metrics  196\\nsystem metrics  197\\niris dataset\\nreference link  6\\nJ\\nJava Virtual Machine (JVM)  209\\nJupyterLab  48\\nK\\nKeras  68\\nKubeflow  113-115, 118\\nKubernetes  113\\nmodels, deploying for batch \\nscoring  161, 162L\\nlocal model registry  156\\nlogistic-based classifier steps\\nsetting up  68-71\\nLogistic Classifier  68\\nlogistic regression-based model\\ncomparing  78, 79\\ntuning, with hyperparameter \\noptimization  79-83\\nM\\nmachine learning\\nbaseline pipeline, developing  37, 38\\nusing, in anomaly detection  29\\nusing, in data mining  29\\nusing, in pair trading  29\\nusing, in sentiment analysis  30\\nusing, in stock forecasting  29\\nMachine Learning Engineering process  6', metadata={'source': 'Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf', 'page': 243}),\n",
       " Document(page_content='7\\nData and Feature \\nManagement\\nIn this chapter, we will add a feature management data layer to the machine learning \\nplatform being built. We will leverage the features of the MLflow Projects module to \\nstructure our data pipeline.\\nSpecifically,\\xa0we will look at the following\\xa0sections in this chapter:\\n• Structuring your data pipeline project\\n• Acquiring stock data\\n• Checking data quality\\n• Managing features\\nIn this chapter, we will acquire relevant data to provide datasets for training. Our primary \\nresource will be the Y ahoo Finance Data for BTC dataset. Alongside that data, we will \\nacquire the following extra datasets.', metadata={'source': 'Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf', 'page': 137}),\n",
       " Document(page_content='4\\n\\n\\n\\n5\\n\\n\\n\\n6\\n\\n\\n\\n7\\n\\n\\n\\n8\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n9\\n\\n\\n\\n10\\n\\n\\n\\n11\\n\\n\\n\\n12\\n\\n\\n\\n13\\n\\n\\n\\n14\\n\\n\\n\\n15\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n16\\n\\n\\n\\n17\\n\\n\\n\\n18\\n\\n\\n\\n19\\n\\n\\n\\n20\\n\\n\\n\\n21\\n\\n\\n\\n22\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n23\\n\\n\\n\\n24\\n\\n\\n\\n25\\n\\n\\n\\n26\\n\\n\\n\\n27\\n\\n\\n\\n28\\n\\n\\n\\n29\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n30\\n\\n\\n\\n31\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nStructural Elements\\n\\nMiscellaneous structural elements you can add to your document, like footnotes, endnotes, dropcaps and the like. \\n\\nFootnotes & Endnotes\\n\\nFootnotes and endnotes are automatically recognized and both are converted to endnotes, with backlinks for maximum ease of use in ebook devices.\\n\\nDropcaps\\n\\nD', metadata={'source': 'Local_data\\\\sample1.docx'}),\n",
       " Document(page_content='ML platform 115\\nDescribing the features of the  \\nML platform 116\\nHigh-level systems architecture 116\\nMLflow and other ecosystem tools 118\\nSummary 118\\nFurther reading 119\\n7\\nData and Feature Management\\nTechnical requirements 122\\nStructuring your data pipeline \\nproject 123\\nAcquiring stock data 127\\nChecking data quality  128Generating a feature set and \\ntraining data 130\\nRunning your end-to-end pipeline 132\\nUsing a feature store 135\\nSummary 138\\nFurther reading 139', metadata={'source': 'Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf', 'page': 7})]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.invoke({\"query\":\"What is the capital of India?\",\"history\":h_1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain= ({\"context\": {\"query\":itemgetter(\"query\") , \"history\":itemgetter(\"history\")} | ensemble_retriever,\n",
    "        \"question\":itemgetter(\"query\")}\n",
    "        | \n",
    "        RunnableParallel({\n",
    "            \"response\":  final_prompt_perspectives | ChatOpenAI(temperature=0) | StrOutputParser() ,\n",
    "            \"context\": itemgetter(\"context\")\n",
    "        })\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': \"I don't know.\",\n",
       " 'context': [Document(page_content=\"About the reviewer\\nHitesh Hinduja is an ardent AI enthusiast working as a Senior Manager in AI at Ola \\nElectric, where he leads a team of 20+ people in the areas of machine learning, deep \\nlearning, statistics, computer vision, natural language processing, and reinforcement \\nlearning. He has filed 14+ patents in India and the US and has numerous research \\npublications under his name. Hitesh has been associated in research roles at India's \\ntop B-schools: Indian School of Business, Hyderabad, and the Indian Institute of \\nManagement, Ahmedabad. He is also actively involved in training and mentoring and has \\nbeen invited as a guest speaker by various corporates and associations across the globe.\", metadata={'source': 'Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf', 'page': 4}),\n",
       "  Document(page_content='1. Install evidently :\\npip install evidently==0.1.17.dev0\\n2. Import the relevant libraries:\\nimport pandas as pd\\nimport numpy as np\\nfrom sklearn import datasets\\nfrom sklearn.model_selection import train_test_split\\nfrom evidently.dashboard import Dashboard\\nfrom evidently.tabs import DataDriftTab, \\nNumTargetDriftTab,CatTargetDriftTab', metadata={'source': 'Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf', 'page': 204}),\n",
       "  Document(page_content='Machine Learning \\nEngineering  \\nwith MLflow\\nManage the end-to-end machine learning life cycle \\nwith MLflow\\nNatu Lauchande\\nBIRMINGHAM—MUMBAI', metadata={'source': 'Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf', 'page': 1}),\n",
       "  Document(page_content='1. We will start by importing the relevant packages:\\nimport mlflow\\nfrom datetime import date\\nfrom dateutil.relativedelta import relativedelta\\nimport pprint\\nimport pandas\\nimport pandas_datareader.data as web\\n2. Next, you should add a function to retrieve the data: \\nif __name__ == \"__main__\":\\n    \\n    with mlflow.start_run(run_name=\"load_raw_data\") as', metadata={'source': 'Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf', 'page': 143}),\n",
       "  Document(page_content='Shyam Sundar\\n5,kavimani steet, Pankajam colony, 3rd cross street, Madurai, India - 625009\\n♂phone+91-9080765574 /envel⌢pemailshyamsundar.2022@gmail.com /linkedinshyamsundar007 /githubShyam-Sundar-7♂laptopPortfolio\\nProjects\\nLLM-Powered Coupon Recommender /github/video |Python, Streamlit, Langchain, OpenAI November 2023\\n•Developed a QA system for e-commerce with personalized coupon recommendations using OpenAI’s LLMs.\\n•Streamlined user interactions through a Streamlit interface and Langchain for real-world scenario simulations.\\n•Incorporated FAISS for refined recommendation processes.\\nPeopleCare Insurance Prediction /github |Python, Jupyter, Azure Cloud, Flask, Docker October 2023\\n•Expanded PeopleCare into vehicle insurance with a predictive model for effective customer targeting.\\n•Thorough analysis of customer behavior and data cleaning for accurate predictive modeling.\\n•Achieved 80% prediction accuracy using LightGBM.', metadata={'source': 'Local_data\\\\resume.pdf', 'page': 0}),\n",
       "  Document(page_content='project dataset. We will execute the following steps in order to implement AutoML for our \\nuse case:\\n1. Let\\'s start by installing the full version of PyCaret, as follows:\\npip install pycaret==2.3.1\\n2. First, we should import the necessary libraries, like so:\\nimport pandas\\nimport pycaret\\n3. Then, we read all the training data, like this:\\ndata=pandas.read_csv(\"training_data.csv\",header=\\'infer\\')', metadata={'source': 'Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf', 'page': 219}),\n",
       "  Document(page_content='this book.\\nPackt Publishing has endeavored to provide trademark information about all of the companies \\nand products mentioned in this book by the appropriate use of capitals. However, Packt Publishing \\ncannot guarantee the accuracy of this information.\\nPublishing Product Manager: Reshma Raman\\nSenior Editor: David Sugarman\\nContent Development Editor : Sean Lobo\\nTechnical Editor: Manikandan Kurup\\nCopy Editor: Safis Editing\\nProject Coordinator: Aparna Ravikumar Nair\\nProofreader : Safis Editing\\nIndexer : Pratik Shirodkar\\nProduction Designer: Sinhayna Bais\\nFirst published: August 2021\\nProduction reference: 1220721\\nPublished by Packt Publishing Ltd.\\nLivery Place\\n35 Livery Street\\nBirmingham\\nB3 2PB, UK.\\nISBN 978-1-80056-079-6\\nwww.packt.com', metadata={'source': 'Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf', 'page': 2}),\n",
       "  Document(page_content='1. Y ou need to set up a Docker file in the root folder of the project, as shown in the \\nfollowing code snippet:\\nFROM continuumio/miniconda3:4.9.2\\nRUN apt-get update && apt-get install build-essential -y\\nRUN pip install \\\\\\n    mlflow==1.18.0 \\\\\\n    pymysql==1.0.2 \\\\\\n    boto3\\nCOPY ./training_project /src\\nWORKDIR /src\\n2. We will start by building and training the image by running the following command:\\ndocker build -t psystock_docker_training_image .', metadata={'source': 'Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf', 'page': 169}),\n",
       "  Document(page_content='•Advanced Numerical Techniques•Machine Learning\\n•Data Science•Deep Learning\\n•Computer Graphics\\nTechnical Skills\\nLanguages : Python, C/C++, SQL (Postgres), Matlab, Latex\\nFrameworks : Pytorch, Tensorflow, Flask, Pytorch Lightning\\nTools/Platform : Tableau, Power Bi, Azure, Git, Jupyter, Docker\\nLibraries : Scikit-Learn, Pandas, Numpy, Matplotlib, Seaborn\\nCertifications\\n•Certified Associate Data Analyst\\n•SQL [Advanced] - Hackerank\\n•Management Consulting Mentorship\\n•Generative AI at SAP•ML for Business professionals using No-Code AI tools\\n•Python [Basic] - Hackerank\\n•Software Engineer Intern -Hackerank\\nPublications\\nSuspicious Event Detection of Cargo Vessels Based on AIS Data at ICDMAI,2023', metadata={'source': 'Local_data\\\\resume.pdf', 'page': 0}),\n",
       "  Document(page_content='1. Copy the contents of the project available in https://github.com/\\nPacktPublishing/Machine-Learning-Engineering-with-MLflow/\\ntree/master/Chapter03/gradflow .\\n2. Start your local environment by running the following command:\\nmake\\n3. Inspect the created environments, like this: \\n$ docker ps \\nThe following screenshot presents three Docker images: the first for Jupyter, the \\nsecond for MLflow, and the third for the PostgreSQL database. The status should \\nshow Up x minutes :\\nFigure 3.3 – Running Docker images', metadata={'source': 'Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf', 'page': 67}),\n",
       "  Document(page_content='Enter the main heading, usually the same as the title.\\n\\nBe bold in stating your key points. Put them in a list:\\n\\nThe first item in your list\\n\\nThe second item; italicize key words\\n\\nImprove your image by including an image.\\n\\nAdd a link to your favorite Web site.\\nBreak up your page with a horizontal rule or two.\\n\\nFinally, link to another page in your own Web site.\\n\\n© Wiley Publishing, 2011', metadata={'source': 'Local_data\\\\sample2.html'}),\n",
       "  Document(page_content='A very popular managed ML and data platform is the Databricks platform, developed \\nby the same company that developed MLflow. We will use in this section the Databricks \\nCommunity Edition version and license targeted for students and personal use. \\nIn order to explore the Databricks platform to develop and share models, you need to \\nexecute the following steps:\\n1. Sign up to Databricks Community Edition at https://community.cloud.\\ndatabricks.com/  and create an account.\\n2. Log in to your account with your just-created credentials.', metadata={'source': 'Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf', 'page': 188}),\n",
       "  Document(page_content='•Central global model collected weight updates from six randomly selected client models, averaging the contributions, and\\ndisseminated the updated global model to all participating clients.\\n•Effected a commendable accuracy rate of 78% upon successful completion of the training process, demonstrating the effectiveness\\nof the federated learning approach in preserving data security and privacy while maintaining model performance.\\nEducation\\nDefence Institute of Advanced Technology Pune, IN\\nM.Tech in Modelling and Simulation, GPA: 7.95 May 2023\\nNational Institute of Technology Tiruchirappalli, IN\\nB.Tech in Chemical Engineering, GPA: 7.65 May 2021\\nRelevant Coursework\\n•Data Structures\\n•Advanced Numerical Techniques•Machine Learning\\n•Data Science•Deep Learning\\n•Computer Graphics\\nTechnical Skills\\nLanguages : Python, C/C++, SQL (Postgres), Matlab, Latex\\nFrameworks : Pytorch, Tensorflow, Flask, Pytorch Lightning\\nTools/Platform : Tableau, Power Bi, Azure, Git, Jupyter, Docker', metadata={'source': 'Local_data\\\\resume.pdf', 'page': 0}),\n",
       "  Document(page_content='Monitoring data drift and model performance      191\\nMonitoring target drift\\nWe will now compare the scored output with the reference training output to look for \\npossible target drift:\\n1. Get the recently scored dataset:\\nproduction_scored_data = \\\\\\npd.read_csv(\"scored_data.csv\", header=None,\\n            names=[ \"day{}\".format(i) for i in \\\\\\n                    range(0,14) ]+[\"target\"] )\\nbcancer_data_and_target_drift = \\\\\\nDashboard(reference_data, production_scored_data,\\n          tabs=[ CatTargetDriftTab])\\nbcancer_data_and_target_drift.save(\\'reports/target_drift.\\nhtml\\')\\n2. Execute the data drift report generation and log the results in MLflow:\\nEXPERIMENT_NAME=\"./reports_target_drift\"\\nmlflow.set_experiment(EXPERIMENT_NAME)\\nwith mlflow.start_run():\\n    model_target_drift = \\\\\\n    Dashboard(reference_data, production_scored_data,\\n              tabs=[CatTargetDriftTab])\\n    model_target_drift.save(EXPERIMENT_NAME+\"/target_\\ndrift.html\")\\n    drift_dashboard._save_to_json(EXPERIMENT_NAME+\"/', metadata={'source': 'Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf', 'page': 207}),\n",
       "  Document(page_content='Quantum Aristoxeni ingenium consumptum videmus in musicis?\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Quid nunc honeste dicit? Tum Torquatus: Prorsus, inquit, assentior; Duo Reges: constructio interrete. Iam in altera philosophiae parte. Sed haec omittamus; Haec para/doca illi, nos admirabilia dicamus. Nihil sane.\\n\\nExpressa vero in iis aetatibus, quae iam confirmatae sunt.\\n\\nSit sane ista voluptas. Non quam nostram quidem, inquit Pomponius iocans; An tu me de L. Sed haec omittamus; Cave putes quicquam esse verius.', metadata={'source': 'Local_data\\\\sample2.docx'}),\n",
       "  Document(page_content='modifications to make it conform to the Docker format acceptable to the MLflow \\ndeployment in production through Kubernetes. The prerequisite of this section is that you \\nhave access to a Kubernetes cluster or can set up a local one. Guides for this can be found \\nat https://kind.sigs.k8s.io/docs/user/quick-start/  or https://\\nminikube.sigs.k8s.io/docs/start/ . \\nY ou will now execute the following steps to deploy your model from the registry  \\nin Kubernetes:\\n1. Prerequisite: Deploy and configure kubectl  (https://kubernetes.io/\\ndocs/reference/kubectl/overview/ ) and link it to your Kubernetes \\ncluster.\\n2. Create a Kubernetes backend configuration file:\\n{\\n  \"kube-context\": \"docker-for-desktop\",\\n  \"repository-uri\": \"username/mlflow-kubernetes-example\",\\n  \"kube-job-template-path\": \"/Users/username/path/to/\\nkubernetes_job_template.yaml\"\\n}', metadata={'source': 'Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf', 'page': 177}),\n",
       "  Document(page_content='Preface     xv\\nGet in touch\\nFeedback from our readers is always welcome.\\nGeneral feedback: If you have questions about any aspect of this book, email us at \\ncustomercare@packtpub.com and mention the book title in the subject of your message.\\nErrata: Although we have taken every care to ensure the accuracy of our content, mistakes \\ndo happen. If you have found a mistake in this book, we would be grateful if you would \\nreport this to us. Please visit www.packtpub.com/support/errata  and fill in  \\nthe form.\\nPiracy: If you come across any illegal copies of our works in any form on the internet, \\nwe would be grateful if you would provide us with the location address or website name. \\nPlease contact us at copyright@packt.com with a link to the material.\\nIf you are interested in becoming an author: If there is a topic that you have expertise  \\nin and you are interested in either writing or contributing to a book, please visit \\nauthors.packtpub.com .\\nShare Your Thoughts', metadata={'source': 'Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf', 'page': 15}),\n",
       "  Document(page_content='def __init__(self):\\n    pass\\n  def predict(self, context, model_input):\\n    return model_input.apply(lambda column: random.\\nrandint(0,1))\\nUnder the notebook  folder in the notebooks/stockpred_randomizer.ipynb  \\nfile, you can follow along with the integration of the preceding code excerpt in our \\nrecently created data science workbench. We will proceed as follows:\\n1. We will first import all the dependencies needed and run the first cell of the \\nnotebook, as follows:\\nFigure 3.9 – MLflow experiment details\\n2. Let’s declare and execute the class outlined in Figure 3.9, represented in the second \\ncell of the notebook, as follows: \\nFigure 3.10 – Notebook cell with the RandomPredictor class declaration', metadata={'source': 'Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf', 'page': 71}),\n",
       "  Document(page_content='City or Town\\n\\nPoint A\\n\\nPoint B\\n\\nPoint C\\n\\nPoint D\\n\\nPoint E\\n\\nPoint A\\n\\n—\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPoint B\\n\\n87\\n\\n—\\n\\n\\n\\n\\n\\n\\n\\nPoint C\\n\\n64\\n\\n56\\n\\n—\\n\\n\\n\\n\\n\\nPoint D\\n\\n37\\n\\n32\\n\\n91\\n\\n—\\n\\n\\n\\nPoint E\\n\\n93\\n\\n35\\n\\n54\\n\\n43\\n\\n—\\n\\n\\n\\nNext, we see a table with special formatting in various locations. Notice how the formatting for the header row and sub header rows is preserved.\\n\\nCollege\\n\\nNew students\\n\\nGraduating students\\n\\nChange\\n\\n\\n\\nUndergraduate\\n\\n\\n\\n\\n\\nCedar University\\n\\n110\\n\\n103\\n\\n+7\\n\\nOak Institute\\n\\n202\\n\\n210\\n\\n-8\\n\\n\\n\\nGraduate\\n\\n\\n\\n\\n\\nCedar University\\n\\n24\\n\\n20\\n\\n+4\\n\\nElm College\\n\\n43\\n\\n53\\n\\n-10\\n\\nTotal\\n\\n998\\n\\n908\\n\\n90\\n\\nSource: Fictitious data, for illustration purposes only\\n\\nNext, we have something a little more complex, a nested table, i.e. a table inside another table. Additionally, the inner table has some of its cells merged. The table is displayed horizontally centered.\\n\\nOne\\n\\nThree\\n\\nTwo\\n\\n\\n\\nFour\\n\\n\\n\\nTo the left is a table inside a table, with some cells merged.', metadata={'source': 'Local_data\\\\sample1.docx'}),\n",
       "  Document(page_content='136     Data and Feature Management\\nIn order to understand how Feast works and how it can fit into your data layer component \\n(code available at https://github.com/PacktPublishing/Machine-\\nLearning-Engineering-with-MLflow/tree/master/Chapter07/\\npsystock_feature_store , execute the following steps:\\n1. Install feast :\\npip install feast==0.10\\n2. Initialize a feature repository:\\nfeast init\\n3. Create your feature definitions by replacing the yaml  file generated automatically:\\nproject: psystock_feature_store\\nregistry: data/registry.db\\nprovider: local\\nonline_store:\\n    path: data/online_store.db\\n4. We will now proceed to import dependencies of the feature definition:\\nfrom google.protobuf.duration_pb2 import Duration\\nfrom feast import Entity, Feature, FeatureView, ValueType\\nfrom feast.data_source import FileSource\\n5. We can now load the feature files:\\ntoken_features = FileSource(\\n    path=\"/data/features.csv\",\\n    event_timestamp_column=\"create_date\",', metadata={'source': 'Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf', 'page': 152}),\n",
       "  Document(page_content='and real-time world and the existence of generic prediction services for API and  \\nbatch systems:\\nFigure.6.3 – Michelangelo architecture \\nThe components that we choose to use as a reference for architecture comparison are  \\nthe following:\\n• Data and feature management: It consists of a centralized data store with all the \\nfeatures that are needed to serve models and train models. The feature data store \\ncan be accessed in real time and in batch. For the batch scenario, they use a database \\nsystem called Hive and for real time, they use Cassandra.', metadata={'source': 'Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf', 'page': 128}),\n",
       "  Document(page_content='on-premises, in the cloud, or local.\\nWe have previously defined the business requirements of the prediction use cases, namely \\ndetection of the movement of cryptocurrency and value prediction. To leverage this and \\nother use cases, the creation of an ML platform is critical to the company.', metadata={'source': 'Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf', 'page': 131}),\n",
       "  Document(page_content='3\\nYour Data Science \\nWorkbench\\nIn this chapter, you will learn about MLflow in the context of creating a local environment \\nso that you can develop your machine learning project locally with the different features \\nprovided by MLflow. This chapter is focused on machine learning engineering, and one \\nof the most important roles of a machine learning engineer is to build up an environment \\nwhere model developers and practitioners can be efficient. We will also demonstrate a \\nhands-on example of how we can use workbenches to accomplish specific tasks.\\nSpecifically,\\xa0we will look at the following\\xa0topics in this chapter:\\xa0\\n• Understanding the value of a data science workbench\\n• Creating your own data science workbench\\n• Using the workbench for stock prediction', metadata={'source': 'Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf', 'page': 59}),\n",
       "  Document(page_content=\"develop our ML platform in the PsyStock company.\\nBased on the work done so far in prototyping models to predict the price of Bitcoin, the \\nbusiness development department of the company decided to start its first product as a \\nPrediction API for cryptocurrencies as they are becoming a popular technology in the \\ncorporate world. A team was assembled that decided to investigate challenges and state-\\nof-the-art platforms, and then architect the company's own platform.\", metadata={'source': 'Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf', 'page': 123}),\n",
       "  Document(page_content='Index   227\\nH\\nHadoop Distributed File \\nSystem (HDFS)  216\\nhyperparameter optimization\\nused, for tuning logistic regression-\\nbased model  79-83\\nHyperText Transfer Protocol (HTTP)  216\\nI\\ninference\\nused, for creating API process  160, 161\\nInformation Technology (IT)  209\\ninfrastructure monitoring and \\nalerting components\\nalerting  198\\nresource metrics  196\\nsystem metrics  197\\niris dataset\\nreference link  6\\nJ\\nJava Virtual Machine (JVM)  209\\nJupyterLab  48\\nK\\nKeras  68\\nKubeflow  113-115, 118\\nKubernetes  113\\nmodels, deploying for batch \\nscoring  161, 162L\\nlocal model registry  156\\nlogistic-based classifier steps\\nsetting up  68-71\\nLogistic Classifier  68\\nlogistic regression-based model\\ncomparing  78, 79\\ntuning, with hyperparameter \\noptimization  79-83\\nM\\nmachine learning\\nbaseline pipeline, developing  37, 38\\nusing, in anomaly detection  29\\nusing, in data mining  29\\nusing, in pair trading  29\\nusing, in sentiment analysis  30\\nusing, in stock forecasting  29\\nMachine Learning Engineering process  6', metadata={'source': 'Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf', 'page': 243}),\n",
       "  Document(page_content='7\\nData and Feature \\nManagement\\nIn this chapter, we will add a feature management data layer to the machine learning \\nplatform being built. We will leverage the features of the MLflow Projects module to \\nstructure our data pipeline.\\nSpecifically,\\xa0we will look at the following\\xa0sections in this chapter:\\n• Structuring your data pipeline project\\n• Acquiring stock data\\n• Checking data quality\\n• Managing features\\nIn this chapter, we will acquire relevant data to provide datasets for training. Our primary \\nresource will be the Y ahoo Finance Data for BTC dataset. Alongside that data, we will \\nacquire the following extra datasets.', metadata={'source': 'Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf', 'page': 137}),\n",
       "  Document(page_content='4\\n\\n\\n\\n5\\n\\n\\n\\n6\\n\\n\\n\\n7\\n\\n\\n\\n8\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n9\\n\\n\\n\\n10\\n\\n\\n\\n11\\n\\n\\n\\n12\\n\\n\\n\\n13\\n\\n\\n\\n14\\n\\n\\n\\n15\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n16\\n\\n\\n\\n17\\n\\n\\n\\n18\\n\\n\\n\\n19\\n\\n\\n\\n20\\n\\n\\n\\n21\\n\\n\\n\\n22\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n23\\n\\n\\n\\n24\\n\\n\\n\\n25\\n\\n\\n\\n26\\n\\n\\n\\n27\\n\\n\\n\\n28\\n\\n\\n\\n29\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n30\\n\\n\\n\\n31\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nStructural Elements\\n\\nMiscellaneous structural elements you can add to your document, like footnotes, endnotes, dropcaps and the like. \\n\\nFootnotes & Endnotes\\n\\nFootnotes and endnotes are automatically recognized and both are converted to endnotes, with backlinks for maximum ease of use in ebook devices.\\n\\nDropcaps\\n\\nD', metadata={'source': 'Local_data\\\\sample1.docx'}),\n",
       "  Document(page_content='ML platform 115\\nDescribing the features of the  \\nML platform 116\\nHigh-level systems architecture 116\\nMLflow and other ecosystem tools 118\\nSummary 118\\nFurther reading 119\\n7\\nData and Feature Management\\nTechnical requirements 122\\nStructuring your data pipeline \\nproject 123\\nAcquiring stock data 127\\nChecking data quality  128Generating a feature set and \\ntraining data 130\\nRunning your end-to-end pipeline 132\\nUsing a feature store 135\\nSummary 138\\nFurther reading 139', metadata={'source': 'Local_data\\\\Natu Lauchande - Machine Learning Engineering with MLflow_ Manage the end-to-end machine learning life cycle with MLflow (2021, Packt Publishing) - libgen.li.pdf', 'page': 7})]}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke({\"query\":\"What is the capital of India?\",\"history\":h_1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
