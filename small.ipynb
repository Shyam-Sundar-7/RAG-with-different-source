{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain_community.document_loaders import UnstructuredPowerPointLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter # type: ignore\n",
    "import boto3\n",
    "import os\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from operator import itemgetter\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "import numpy as np\n",
    "from dotenv  import load_dotenv\n",
    "load_dotenv()\n",
    "import shutil\n",
    "\n",
    "def load_file(file_name):\n",
    "    loader=[]\n",
    "    print(file_name.split(\".\")[-1])\n",
    "    if file_name.split('.')[-1] == \"pptx\":\n",
    "        loader = UnstructuredPowerPointLoader(file_name).load()\n",
    "    elif file_name.split('.')[-1] == \"pdf\":\n",
    "        loader = PyPDFLoader(file_name).load()    \n",
    "    elif file_name.split('.')[-1] == \"docx\":\n",
    "        loader = Docx2txtLoader(file_name).load()\n",
    "    elif file_name.split('.')[-1] == \"html\":\n",
    "        loader = UnstructuredHTMLLoader(file_name).load()\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        # separator=\"\\n\\n\",\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False\n",
    "        )\n",
    "    pages = text_splitter.split_documents(loader)\n",
    "    return pages\n",
    "\n",
    "\n",
    "def file_to_chunks(folder):\n",
    "    pages=[]\n",
    "    for file_name in os.listdir(f\"{folder}\"):\n",
    "        pages.extend(load_file(f\"{folder}\\\\{file_name}\"))\n",
    "    if folder != \"Local_data\":\n",
    "        shutil.rmtree(f\"{folder}\")\n",
    "    return pages\n",
    "\n",
    "def azure_data_download(AZURE_CONNECTION_STRING,CONTAINER_NAME):\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(AZURE_CONNECTION_STRING)\n",
    "    container_client = blob_service_client.get_container_client(CONTAINER_NAME)\n",
    "    if not os.path.exists(\"Azure_data\"):\n",
    "        os.mkdir(\"Azure_data\")\n",
    "    for file_name in container_client.list_blobs():\n",
    "        blob_client = container_client.get_blob_client(file_name)\n",
    "        with open(f\"Azure_data\\\\{file_name.name}\", \"wb\") as file:\n",
    "            data = blob_client.download_blob().readall()\n",
    "            file.write(data)\n",
    "\n",
    "\n",
    "def aws(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, BUCKET_NAME,object_name):\n",
    "        # Create an S3 client\n",
    "    s3 = boto3.client('s3',\n",
    "                    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "                    aws_secret_access_key=AWS_SECRET_ACCESS_KEY)\n",
    "\n",
    "    # List objects in the bucket\n",
    "    response = s3.list_objects_v2(Bucket=BUCKET_NAME)\n",
    "\n",
    "    if not os.path.exists(\"S3_data\"):\n",
    "        os.mkdir(\"S3_data\")\n",
    "\n",
    "    # Download files in the 'data' object\n",
    "    for i in response.get('Contents',[]):\n",
    "        if i['Key'].split('/')[-1] != \"\" and i['Key'].split('/')[0] == object_name:\n",
    "            # print(i['Key'])\n",
    "            file_path = os.path.join(\"S3_data\", i['Key'].split('/')[-1])\n",
    "            # print(file_path)\n",
    "            s3.download_file(BUCKET_NAME, i['Key'], file_path)\n",
    "\n",
    "\n",
    "def l(h):\n",
    "    d=\"\"\n",
    "    for i in h:\n",
    "        for _,j in i.items():\n",
    "            d=d+f\"{j} \\n\"\n",
    "    if d == \"\":\n",
    "        d = \"No history found\"\n",
    "    return d\n",
    "\n",
    "\n",
    "def generate_queries(query):\n",
    "        \n",
    "    # Multi Query: Different Perspectives\n",
    "    with open(\"history.json\", \"r\") as f:\n",
    "        h=json.load(f)\n",
    "    h_1 =l(h)\n",
    "    # print(h_1)\n",
    "    # print(h)\n",
    "    # print(type(h))\n",
    "    template = f\"\"\"You are an AI language model assistant. Your task is to generate four different versions of the given user question to retrieve relevant documents from a vector database. By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search.\n",
    "        Additionally, look for the relevant information from the history to provide more contextually accurate variations. Provide these alternative questions separated by newlines.\n",
    "        \n",
    "        history : {h_1}\n",
    "        Original question: {query}\n",
    "        \"\"\"\n",
    "    # print(template)\n",
    "    # print(h_1)\n",
    "    prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "    generate_querie = (\n",
    "        prompt_perspectives\n",
    "        | ChatOpenAI(temperature=0) \n",
    "        | StrOutputParser() \n",
    "        | (lambda x: x.split(\"\\n\"))\n",
    "        | (lambda x: [query] + x)\n",
    "    )\n",
    "    return generate_querie \n",
    "\n",
    "def keyword_extractor():\n",
    "    prompt=\"\"\"\n",
    "    You are an AI language model assistant. Your task is to help the user identify key terms in their query.\n",
    "\n",
    "    Please list the main keywords you want to extract from your query.\n",
    "\n",
    "    query: {query}\n",
    "    \"\"\"\n",
    "    prompt_perspectives=ChatPromptTemplate.from_template(prompt)\n",
    "    generate_querie = (\n",
    "        prompt_perspectives \n",
    "        | ChatOpenAI(temperature=0) \n",
    "        | StrOutputParser() )\n",
    "    return generate_querie\n",
    "\n",
    "def _get(a):\n",
    "    dd=[]\n",
    "    for s in a:\n",
    "        dd.extend(s)\n",
    "    return dd\n",
    "\n",
    "def get_unique_documents(doc_list):\n",
    "    seen_content = set()\n",
    "    unique_documents = []\n",
    "    \n",
    "    for doc in doc_list:\n",
    "        content = doc.page_content\n",
    "        if content not in seen_content:\n",
    "            seen_content.add(content)\n",
    "            unique_documents.append(doc)\n",
    "    \n",
    "    del seen_content\n",
    "    \n",
    "    return unique_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an AI language model assistant. Your task is to generate four different versions of the given user question to retrieve relevant documents from a vector database. By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search.\n",
      "Additionally, look for the relevant information from the history to provide more contextually accurate variations. Provide these alternative questions separated by newlines.\n",
      "\n",
      "history : No history found\n",
      "Original question: {query}\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "langchain_core.prompts.prompt.PromptTemplate() got multiple values for keyword argument 'input_variables'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(template)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# print(h_1)\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m prompt_perspectives \u001b[38;5;241m=\u001b[39m \u001b[43mChatPromptTemplate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43minput_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhistory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shyams\\Downloads\\RAG with different source\\.venv\\Lib\\site-packages\\langchain_core\\prompts\\chat.py:890\u001b[0m, in \u001b[0;36mChatPromptTemplate.from_template\u001b[1;34m(cls, template, **kwargs)\u001b[0m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_template\u001b[39m(\u001b[38;5;28mcls\u001b[39m, template: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatPromptTemplate:\n\u001b[0;32m    878\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a chat prompt template from a template string.\u001b[39;00m\n\u001b[0;32m    879\u001b[0m \n\u001b[0;32m    880\u001b[0m \u001b[38;5;124;03m    Creates a chat template consisting of a single message assumed to be from\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    888\u001b[0m \u001b[38;5;124;03m        A new instance of this class.\u001b[39;00m\n\u001b[0;32m    889\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 890\u001b[0m     prompt_template \u001b[38;5;241m=\u001b[39m \u001b[43mPromptTemplate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    891\u001b[0m     message \u001b[38;5;241m=\u001b[39m HumanMessagePromptTemplate(prompt\u001b[38;5;241m=\u001b[39mprompt_template)\n\u001b[0;32m    892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_messages([message])\n",
      "File \u001b[1;32mc:\\Users\\shyams\\Downloads\\RAG with different source\\.venv\\Lib\\site-packages\\langchain_core\\prompts\\prompt.py:252\u001b[0m, in \u001b[0;36mPromptTemplate.from_template\u001b[1;34m(cls, template, template_format, partial_variables, **kwargs)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _partial_variables:\n\u001b[0;32m    248\u001b[0m     input_variables \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    249\u001b[0m         var \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m input_variables \u001b[38;5;28;01mif\u001b[39;00m var \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _partial_variables\n\u001b[0;32m    250\u001b[0m     ]\n\u001b[1;32m--> 252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[0;32m    253\u001b[0m     input_variables\u001b[38;5;241m=\u001b[39minput_variables,\n\u001b[0;32m    254\u001b[0m     template\u001b[38;5;241m=\u001b[39mtemplate,\n\u001b[0;32m    255\u001b[0m     template_format\u001b[38;5;241m=\u001b[39mtemplate_format,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    256\u001b[0m     partial_variables\u001b[38;5;241m=\u001b[39m_partial_variables,\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    258\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: langchain_core.prompts.prompt.PromptTemplate() got multiple values for keyword argument 'input_variables'"
     ]
    }
   ],
   "source": [
    "with open(\"history.json\", \"r\") as f:\n",
    "    h=json.load(f)\n",
    "h_1 =l(h)\n",
    "# print(h_1)\n",
    "# print(h)\n",
    "# print(type(h))\n",
    "template = f\"\"\"You are an AI language model assistant. Your task is to generate four different versions of the given user question to retrieve relevant documents from a vector database. By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search.\n",
    "Additionally, look for the relevant information from the history to provide more contextually accurate variations. Provide these alternative questions separated by newlines.\n",
    "\n",
    "history : \"\"\"\n",
    "template=template+h_1\n",
    "template=template+\"\"\"\n",
    "Original question: {query}\n",
    "\"\"\"\n",
    "\n",
    "print(template)\n",
    "# print(h_1)\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template,input_variables=[\"query\",\"history\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=[], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are an AI language model assistant. Your task is to generate four different versions of the given user question to retrieve relevant documents from a vector database. By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search.\\nAdditionally, look for the relevant information from the history to provide more contextually accurate variations. Provide these alternative questions separated by newlines.\\n\\nhistory : No history found\\nOriginal question: What are the health benefits of green tea?\\n'))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_perspectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: You are an AI language model assistant. Your task is to generate four different versions of the given user question to retrieve relevant documents from a vector database. By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search.\n",
      "        Additionally, look for the relevant information from the history to provide more contextually accurate variations. Provide these alternative questions separated by newlines.\n",
      "        \n",
      "        history : No history found\n",
      "        Original question: tell me more about the shyam sundar's project\n",
      "        \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mgenerate_queries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQuery\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39minvoke(Query)\n",
      "Cell \u001b[1;32mIn[7], line 130\u001b[0m, in \u001b[0;36mgenerate_queries\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m    126\u001b[0m formatted_prompt \u001b[38;5;241m=\u001b[39m prompt_perspectives\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minput_data)\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28mprint\u001b[39m(formatted_prompt)\n\u001b[0;32m    129\u001b[0m generate_querie \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 130\u001b[0m     \u001b[43mformatted_prompt\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[0;32m    132\u001b[0m     \u001b[38;5;241m|\u001b[39m StrOutputParser() \n\u001b[0;32m    133\u001b[0m     \u001b[38;5;241m|\u001b[39m (\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;241m|\u001b[39m (\u001b[38;5;28;01mlambda\u001b[39;00m x: [query] \u001b[38;5;241m+\u001b[39m x)\n\u001b[0;32m    135\u001b[0m )\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m generate_querie\n",
      "File \u001b[1;32mc:\\Users\\shyams\\Downloads\\RAG with different source\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:433\u001b[0m, in \u001b[0;36mRunnable.__ror__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__ror__\u001b[39m(\n\u001b[0;32m    424\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    425\u001b[0m     other: Union[\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    430\u001b[0m     ],\n\u001b[0;32m    431\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RunnableSerializable[Other, Output]:\n\u001b[0;32m    432\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compose this runnable with another object to create a RunnableSequence.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m RunnableSequence(\u001b[43mcoerce_to_runnable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\shyams\\Downloads\\RAG with different source\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4977\u001b[0m, in \u001b[0;36mcoerce_to_runnable\u001b[1;34m(thing)\u001b[0m\n\u001b[0;32m   4975\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Runnable[Input, Output], RunnableParallel(thing))\n\u001b[0;32m   4976\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4977\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   4978\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a Runnable, callable or dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4979\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstead got an unsupported type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(thing)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4980\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'str'>"
     ]
    }
   ],
   "source": [
    "generate_queries(Query).invoke(Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
